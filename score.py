import os
import json
import traceback
import logging
from transformers import pipeline, AutoTokenizer
from azureml.core.model import Model
from azureml.monitoring import ModelDataCollector

# Define global variables
global model, tokenizer, prediction_dc

# Initialize logging
logging.basicConfig(level=logging.INFO)

# Azure ML Model Data Collector (for monitoring)
prediction_dc = None

def init():
    """
    Initialize required models and load them from the Azure ML model registry.
    """
    global model, tokenizer, prediction_dc

    try:
        # Define model name and version as registered in Azure ML
        model_name = "transformer_cicd"  # Model name in Azure ML
        model_version = "16"  # Specify the version, or leave blank for the latest
        
        # Retrieve the model path from Azure ML
        logging.info(f"Fetching model path for model: {model_name}, version: {model_version}")
        model_path = Model.get_model_path(model_name, version=model_version)
        logging.info(f"Model path retrieved: {model_path}")

        # Load the Hugging Face pipeline and tokenizer from the model path
        logging.info("Loading Hugging Face pipeline and tokenizer...")
        model = pipeline("text-generation", model=model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        logging.info("Model and tokenizer loaded successfully.")

        # Initialize Model Data Collector for monitoring
        prediction_dc = ModelDataCollector(
            model_name, 
            designation="predictions", 
            feature_names=["Input Text", "Generated Text"]
        )
        logging.info("Model Data Collector initialized for monitoring.")
    
    except Exception as e:
        logging.error(f"Error initializing the model: {e}")
        traceback.print_exc()

def create_response(predicted_output):
    """
    Create a response object for the API output.

    Arguments:
        predicted_output: The output generated by the model.
    
    Returns:
        JSON response object with the generated text.
    """
    response = {"generated_text": predicted_output}
    return json.dumps({"output": response})

def run(raw_data):
    """
    Perform inference using the Hugging Face model.

    Arguments:
        raw_data: Input data for prediction in JSON format (with a "text" field).
    
    Returns:
        JSON response with the predicted result or an error message.
    """
    try:
        # Parse the incoming raw data
        data = json.loads(raw_data)
        user_input = data.get('text', None)

        if not user_input:
            logging.warning("No input text provided in the request.")
            return json.dumps({"error": "No input text provided."})

        # Generate text using the Hugging Face pipeline
        logging.info(f"Received input text: {user_input}")
        generated_output = model(user_input, max_length=50, num_return_sequences=1)[0]['generated_text']
        logging.info(f"Generated output: {generated_output}")

        # Collect prediction for monitoring
        prediction_dc.collect([user_input, generated_output])

        # Return the generated text as the response
        return create_response(generated_output)
    
    except Exception as e:
        logging.error(f"Error during inference: {e}")
        traceback.print_exc()
        return json.dumps({"error": "An error occurred during prediction."})
